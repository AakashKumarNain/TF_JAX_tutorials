{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a02c50f",
   "metadata": {
    "papermill": {
     "duration": 0.02369,
     "end_time": "2021-07-27T14:50:00.257982",
     "exception": false,
     "start_time": "2021-07-27T14:50:00.234292",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/google/jax/main/images/jax_logo_250px.png\" width=\"300\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "I hope you have been liking the tutorials so far. This is the fourth tutorial in this series and today, we will be starting with **JAX**. If you haven't looked at the previous tutorials, I highly suggest going through them once. Here are the links:\n",
    "\n",
    "1. [TF_JAX_Tutorials - Part 1](https://www.kaggle.com/aakashnain/tf-jax-tutorials-part1)\n",
    "2. [TF_JAX_Tutorials - Part 2](https://www.kaggle.com/aakashnain/tf-jax-tutorials-part2)\n",
    "3. [TF_JAX_Tutorials - Part 3](https://www.kaggle.com/aakashnain/tf-jax-tutorials-part3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913c61ba",
   "metadata": {
    "papermill": {
     "duration": 0.021974,
     "end_time": "2021-07-27T14:50:00.304537",
     "exception": false,
     "start_time": "2021-07-27T14:50:00.282563",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What is JAX anyway?\n",
    "\n",
    "**JAX** is a framework that is specifically suited for Machine Learning Research (more on this later). A few points about JAX:\n",
    "1. It's just like `numpy` but uses a compiler (XLA) to compile native Numpy code, and runs on acceleartors (GPU/TPU)\n",
    "2. For automatic differentiation, JAX uses `Autograd`. It automatically differentiate native Python and Numpy code.\n",
    "3. JAX is used to express numerical programs as compositions but with certain constraints e.g. JAX transformation and compilation are designed to work only on Python functions that are functionally pure. A function is pure if it always returns the same value when invoked with same arguments, and the function has no-side affect e.g. chaning the state of a non-local variables\n",
    "4. In terms of syntax, JAX is very very similar to numpy but there are subtle differences that you should be aware of. We will look into it in a bit.\n",
    "\n",
    "Let's take a few examples to see JAX in-play!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad7a3ba9",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-07-27T14:50:00.353497Z",
     "iopub.status.busy": "2021-07-27T14:50:00.352302Z",
     "iopub.status.idle": "2021-07-27T14:50:02.190810Z",
     "shell.execute_reply": "2021-07-27T14:50:02.189857Z",
     "shell.execute_reply.started": "2021-07-21T11:32:49.17212Z"
    },
    "papermill": {
     "duration": 1.864229,
     "end_time": "2021-07-27T14:50:02.191015",
     "exception": false,
     "start_time": "2021-07-27T14:50:00.326786",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import jax\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "\n",
    "%config IPCompleter.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fb83a54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T14:50:02.251103Z",
     "iopub.status.busy": "2021-07-27T14:50:02.250194Z",
     "iopub.status.idle": "2021-07-27T14:50:02.451903Z",
     "shell.execute_reply": "2021-07-27T14:50:02.452487Z",
     "shell.execute_reply.started": "2021-07-21T11:32:50.951613Z"
    },
    "papermill": {
     "duration": 0.237775,
     "end_time": "2021-07-27T14:50:02.452703",
     "exception": false,
     "start_time": "2021-07-27T14:50:02.214928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array created using numpy:  [0 1 2 3 4 5 6 7 8 9]\n",
      "Array created using JAX:  [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "# We will create two arrays, one with numpy and other with jax\n",
    "# to check the common things and the differences\n",
    "\n",
    "array_numpy = np.arange(10, dtype=np.int32)\n",
    "array_jax = jnp.arange(10, dtype=jnp.int32)\n",
    "\n",
    "print(\"Array created using numpy: \", array_numpy)\n",
    "print(\"Array created using JAX: \", array_jax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4e5721e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T14:50:02.503091Z",
     "iopub.status.busy": "2021-07-27T14:50:02.501754Z",
     "iopub.status.idle": "2021-07-27T14:50:02.508221Z",
     "shell.execute_reply": "2021-07-27T14:50:02.508888Z",
     "shell.execute_reply.started": "2021-07-21T11:32:51.190589Z"
    },
    "papermill": {
     "duration": 0.033444,
     "end_time": "2021-07-27T14:50:02.509121",
     "exception": false,
     "start_time": "2021-07-27T14:50:02.475677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array_numpy is of type : <class 'numpy.ndarray'>\n",
      "array_jax is of type : <class 'jaxlib.xla_extension.DeviceArray'>\n"
     ]
    }
   ],
   "source": [
    "# What types of array are these?\n",
    "print(f\"array_numpy is of type : {type(array_numpy)}\")\n",
    "print(f\"array_jax is of type : {type(array_jax)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f743886",
   "metadata": {
    "papermill": {
     "duration": 0.023337,
     "end_time": "2021-07-27T14:50:02.555536",
     "exception": false,
     "start_time": "2021-07-27T14:50:02.532199",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "So, `array_numpy` is an object of **`ndarray`** while `array_jax` is an object of **`DeviceArray`**. Before discussing anything else, let's dive into **`DeviceArray`** and see what makes **DeviceArray** so special.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a0923c",
   "metadata": {
    "papermill": {
     "duration": 0.023864,
     "end_time": "2021-07-27T14:50:02.602414",
     "exception": false,
     "start_time": "2021-07-27T14:50:02.578550",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# DeviceArray\n",
    "\n",
    "Following are the points that you should know about **`DeviceArray`**:\n",
    "1. It is the core underlying JAX array object, similar to `ndarray` but with subtle differences (more on this in the examples below)\n",
    "2. Unlike `ndarray`, `DeviceArray` is backed by a memory buffer on a single device (CPU/GPU/TPU)\n",
    "3. It is **device-agnostic** i.e. JAX doesn't need to track the device on which the array is present, and can avoid data transfers\n",
    "4. Because it is device agnostic, this makes it easy to run the same JAX code on CPU, GPU, or TPU with no code changes\n",
    "5. `DeviceArray` is **lazy** i.e. the value of a JAX `DeviceArray` isn't immediately available and is only pulled when requested.\n",
    "6. Even though `DeviceArray` is lazy, you can still do operations like inspecting the shape or type of a DeviceArray without waiting for the computation that produced it to complete. We can even pass it to another JAX computation. (The examples will make it more clear)\n",
    "\n",
    "The two properties **lazy evaluation**, and being **device-agnostic** give **`DeviceArray`** a huge advantage. You will see this in the future tutorials as we dive deeper and deeper into complex things like model building, optimization, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb1aae4",
   "metadata": {
    "papermill": {
     "duration": 0.022622,
     "end_time": "2021-07-27T14:50:02.648079",
     "exception": false,
     "start_time": "2021-07-27T14:50:02.625457",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Numpy vs JAX-numpy\n",
    "\n",
    "`jax numpy` is very very similar to `numpy` in terms of API. *Most of the operations* that you do in numpy are also available in jax numpy with similar semantics. I am just listing down a few operations to showcase this but there are many more. Please check the [docs](https://jax.readthedocs.io/en/latest/jax.numpy.html) to see the list of functions that are available.\n",
    "\n",
    "**Note:** Not all Numpy functions are implemented in JAX numpy (..yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8e4c218",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T14:50:02.705298Z",
     "iopub.status.busy": "2021-07-27T14:50:02.704107Z",
     "iopub.status.idle": "2021-07-27T14:50:02.728313Z",
     "shell.execute_reply": "2021-07-27T14:50:02.728825Z",
     "shell.execute_reply.started": "2021-07-21T11:32:56.019066Z"
    },
    "papermill": {
     "duration": 0.058205,
     "end_time": "2021-07-27T14:50:02.729072",
     "exception": false,
     "start_time": "2021-07-27T14:50:02.670867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum element in ndarray: 9\n",
      "Maximum element in DeviceArray: 9\n"
     ]
    }
   ],
   "source": [
    "# Find the max element. Similarly you can find `min` as well\n",
    "print(f\"Maximum element in ndarray: {array_numpy.max()}\")\n",
    "print(f\"Maximum element in DeviceArray: {array_jax.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fe43dea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T14:50:02.781677Z",
     "iopub.status.busy": "2021-07-27T14:50:02.780777Z",
     "iopub.status.idle": "2021-07-27T14:50:02.801286Z",
     "shell.execute_reply": "2021-07-27T14:50:02.802021Z",
     "shell.execute_reply.started": "2021-07-21T11:33:00.176707Z"
    },
    "papermill": {
     "duration": 0.048342,
     "end_time": "2021-07-27T14:50:02.802267",
     "exception": false,
     "start_time": "2021-07-27T14:50:02.753925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape of ndarray:  (10,)\n",
      "Original shape of DeviceArray:  (10,)\n",
      "\n",
      "New shape of ndarray:  (10, 1)\n",
      "New shape of DeviceArray:  (10, 1)\n"
     ]
    }
   ],
   "source": [
    "# Reshaping\n",
    "print(\"Original shape of ndarray: \", array_numpy.shape)\n",
    "print(\"Original shape of DeviceArray: \", array_jax.shape)\n",
    "\n",
    "array_numpy = array_numpy.reshape(-1, 1)\n",
    "array_jax = array_jax.reshape(-1, 1)\n",
    "\n",
    "print(\"\\nNew shape of ndarray: \", array_numpy.shape)\n",
    "print(\"New shape of DeviceArray: \", array_jax.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b92f491",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T14:50:02.859414Z",
     "iopub.status.busy": "2021-07-27T14:50:02.858616Z",
     "iopub.status.idle": "2021-07-27T14:50:02.978300Z",
     "shell.execute_reply": "2021-07-27T14:50:02.977656Z",
     "shell.execute_reply.started": "2021-07-21T11:33:02.140835Z"
    },
    "papermill": {
     "duration": 0.151977,
     "end_time": "2021-07-27T14:50:02.978474",
     "exception": false,
     "start_time": "2021-07-27T14:50:02.826497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absoulte pairwise difference in ndarray\n",
      "[[0 1 2 3 4 5 6 7 8 9]\n",
      " [1 0 1 2 3 4 5 6 7 8]\n",
      " [2 1 0 1 2 3 4 5 6 7]\n",
      " [3 2 1 0 1 2 3 4 5 6]\n",
      " [4 3 2 1 0 1 2 3 4 5]\n",
      " [5 4 3 2 1 0 1 2 3 4]\n",
      " [6 5 4 3 2 1 0 1 2 3]\n",
      " [7 6 5 4 3 2 1 0 1 2]\n",
      " [8 7 6 5 4 3 2 1 0 1]\n",
      " [9 8 7 6 5 4 3 2 1 0]]\n",
      "\n",
      "Absoulte pairwise difference in DeviceArray\n",
      "[[0 1 2 3 4 5 6 7 8 9]\n",
      " [1 0 1 2 3 4 5 6 7 8]\n",
      " [2 1 0 1 2 3 4 5 6 7]\n",
      " [3 2 1 0 1 2 3 4 5 6]\n",
      " [4 3 2 1 0 1 2 3 4 5]\n",
      " [5 4 3 2 1 0 1 2 3 4]\n",
      " [6 5 4 3 2 1 0 1 2 3]\n",
      " [7 6 5 4 3 2 1 0 1 2]\n",
      " [8 7 6 5 4 3 2 1 0 1]\n",
      " [9 8 7 6 5 4 3 2 1 0]]\n",
      "\n",
      "Are all the values same? True\n"
     ]
    }
   ],
   "source": [
    "# Absoulte pairwise difference\n",
    "print(\"Absoulte pairwise difference in ndarray\")\n",
    "print(np.abs(array_numpy - array_numpy.T))\n",
    "\n",
    "print(\"\\nAbsoulte pairwise difference in DeviceArray\")\n",
    "print(jnp.abs(array_jax - array_jax.T))\n",
    "\n",
    "# Are they equal?\n",
    "print(\"\\nAre all the values same?\", end=\" \")\n",
    "print(jnp.alltrue(np.abs(array_numpy - array_numpy.T) == jnp.abs(array_jax - array_jax.T)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dbb2261",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T14:50:03.038069Z",
     "iopub.status.busy": "2021-07-27T14:50:03.036619Z",
     "iopub.status.idle": "2021-07-27T14:50:03.063207Z",
     "shell.execute_reply": "2021-07-27T14:50:03.063762Z",
     "shell.execute_reply.started": "2021-07-21T11:33:03.999811Z"
    },
    "papermill": {
     "duration": 0.059491,
     "end_time": "2021-07-27T14:50:03.063980",
     "exception": false,
     "start_time": "2021-07-27T14:50:03.004489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix multiplication of ndarray\n",
      "[[ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  1  2  3  4  5  6  7  8  9]\n",
      " [ 0  2  4  6  8 10 12 14 16 18]\n",
      " [ 0  3  6  9 12 15 18 21 24 27]\n",
      " [ 0  4  8 12 16 20 24 28 32 36]\n",
      " [ 0  5 10 15 20 25 30 35 40 45]\n",
      " [ 0  6 12 18 24 30 36 42 48 54]\n",
      " [ 0  7 14 21 28 35 42 49 56 63]\n",
      " [ 0  8 16 24 32 40 48 56 64 72]\n",
      " [ 0  9 18 27 36 45 54 63 72 81]]\n",
      "\n",
      "Matrix multiplication of DeviceArray\n",
      "[[ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  1  2  3  4  5  6  7  8  9]\n",
      " [ 0  2  4  6  8 10 12 14 16 18]\n",
      " [ 0  3  6  9 12 15 18 21 24 27]\n",
      " [ 0  4  8 12 16 20 24 28 32 36]\n",
      " [ 0  5 10 15 20 25 30 35 40 45]\n",
      " [ 0  6 12 18 24 30 36 42 48 54]\n",
      " [ 0  7 14 21 28 35 42 49 56 63]\n",
      " [ 0  8 16 24 32 40 48 56 64 72]\n",
      " [ 0  9 18 27 36 45 54 63 72 81]]\n"
     ]
    }
   ],
   "source": [
    "# Matrix multiplication\n",
    "print(\"Matrix multiplication of ndarray\")\n",
    "print(np.dot(array_numpy, array_numpy.T))\n",
    "\n",
    "print(\"\\nMatrix multiplication of DeviceArray\")\n",
    "print(jnp.dot(array_jax, array_jax.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ee09f5",
   "metadata": {
    "papermill": {
     "duration": 0.024741,
     "end_time": "2021-07-27T14:50:03.114099",
     "exception": false,
     "start_time": "2021-07-27T14:50:03.089358",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now, let's take a look at some of the things that you can do in Numpy but not in Jax-numpy and vice-versa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df66eeeb",
   "metadata": {
    "papermill": {
     "duration": 0.026031,
     "end_time": "2021-07-27T14:50:03.165876",
     "exception": false,
     "start_time": "2021-07-27T14:50:03.139845",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Immutability\n",
    "\n",
    "JAX arrays are **immutable**, just like [**TensorFlow tensors**](https://www.kaggle.com/aakashnain/tf-jax-tutorials-part1). Meaning, JAX arrays don't support `item assignment` as you do in `ndarray`. Let's take an example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59cb2ca0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T14:50:03.225396Z",
     "iopub.status.busy": "2021-07-27T14:50:03.224560Z",
     "iopub.status.idle": "2021-07-27T14:50:03.243198Z",
     "shell.execute_reply": "2021-07-27T14:50:03.242525Z",
     "shell.execute_reply.started": "2021-07-21T11:33:08.427786Z"
    },
    "papermill": {
     "duration": 0.052066,
     "end_time": "2021-07-27T14:50:03.243370",
     "exception": false,
     "start_time": "2021-07-27T14:50:03.191304",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original ndarray:  [0 1 2 3 4]\n",
      "Original DeviceArray:  [0 1 2 3 4]\n",
      "\n",
      "Modified ndarray:  [ 0  1  2  3 10]\n",
      "\n",
      "Trying to modify DeviceArray->  TypeError '<class 'jaxlib.xla_extension.DeviceArray'>' object does not support item assignment. JAX arrays are immutable; perhaps you want jax.ops.index_update or jax.ops.index_add instead?\n"
     ]
    }
   ],
   "source": [
    "array1 = np.arange(5, dtype=np.int32)\n",
    "array2 = jnp.arange(5, dtype=jnp.int32)\n",
    "\n",
    "print(\"Original ndarray: \", array1)\n",
    "print(\"Original DeviceArray: \", array2)\n",
    "\n",
    "# Item assignment\n",
    "array1[4] = 10\n",
    "print(\"\\nModified ndarray: \", array1)\n",
    "print(\"\\nTrying to modify DeviceArray-> \", end=\" \")\n",
    "\n",
    "try:\n",
    "    array2[4] = 10\n",
    "    print(\"Modified DeviceArray: \", array2)\n",
    "except Exception as ex:\n",
    "    print(type(ex).__name__, ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1a29e8",
   "metadata": {
    "papermill": {
     "duration": 0.025056,
     "end_time": "2021-07-27T14:50:03.294268",
     "exception": false,
     "start_time": "2021-07-27T14:50:03.269212",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This situation is exactly the same as we have with TensorFlow Tensors. Similar to `tf.tensor_scatter_nd_update` in TensorFlow, we have [Indexed update operators](https://jax.readthedocs.io/en/latest/jax.ops.html#indexed-update-operators)( earlier there used to be [**jax.ops.index_update(..)**](https://jax.readthedocs.io/en/latest/_autosummary/jax.ops.index_update.html#jax.ops.index_update) but it's deprecated now). The syntax is pretty simple e.g. `DeviceArray.at[idx].op(val)`. This doesn't modify the original array though, it returns a new array with the elements updated as specified\n",
    "\n",
    "\n",
    "A question that naturally comes to mind? **Why immutability?** The thing is that JAX relies on **pure functions**. Allowing item-assignment or in-place updates is the opposite of that philosophy.\n",
    "\n",
    "But then why TF Tensors are immutable as it doesn't need pure functions? If you are doing any optimization on a DAG, it is highly advisable to avoid things that change the state of an op used in the computation to avoid any side effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2816acb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T14:50:03.352666Z",
     "iopub.status.busy": "2021-07-27T14:50:03.351905Z",
     "iopub.status.idle": "2021-07-27T14:50:03.439908Z",
     "shell.execute_reply": "2021-07-27T14:50:03.440450Z",
     "shell.execute_reply.started": "2021-07-21T11:34:14.951493Z"
    },
    "papermill": {
     "duration": 0.120735,
     "end_time": "2021-07-27T14:50:03.440665",
     "exception": false,
     "start_time": "2021-07-27T14:50:03.319930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DeviceArray:  [0 1 2 3 4]\n",
      "Modified DeviceArray:  [ 0  1  2  3 10]\n"
     ]
    }
   ],
   "source": [
    "# Modifying DeviceArray elements at specific index/indices\n",
    "array2_modified = array2.at[4].set(10)\n",
    "\n",
    "# Equivalent => array2_modified = jax.ops.index_update(array2, 4, 10)\n",
    "\n",
    "print(\"Original DeviceArray: \", array2)\n",
    "print(\"Modified DeviceArray: \", array2_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8ebe42c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T14:50:03.504345Z",
     "iopub.status.busy": "2021-07-27T14:50:03.503390Z",
     "iopub.status.idle": "2021-07-27T14:50:03.569331Z",
     "shell.execute_reply": "2021-07-27T14:50:03.569839Z",
     "shell.execute_reply.started": "2021-07-21T11:34:16.529997Z"
    },
    "papermill": {
     "duration": 0.101754,
     "end_time": "2021-07-27T14:50:03.570110",
     "exception": false,
     "start_time": "2021-07-27T14:50:03.468356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3 10]\n",
      "[ 0  1  2  3 20]\n",
      "[ 0  1  2  3 -1]\n",
      "\n",
      "Equivalent but deprecatd\n",
      "[ 0  1  2  3 10]\n",
      "[ 0  1  2  3 20]\n",
      "[ 0  1  2  3 -1]\n"
     ]
    }
   ],
   "source": [
    "# Of course, updates come in many forms!\n",
    "# Of course, updates come in many forms!\n",
    "print(array2.at[4].add(6))\n",
    "print(array2.at[4].max(20))\n",
    "print(array2.at[4].min(-1))\n",
    "\n",
    "# Equivalent but depecated. Just to showcase the similarity to tf scatter_nd_update\n",
    "print(\"\\nEquivalent but deprecatd\")\n",
    "print(jax.ops.index_add(array2, 4, 6))\n",
    "print(jax.ops.index_max(array2, 4, 20))\n",
    "print(jax.ops.index_min(array2, 4, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1a15b0",
   "metadata": {
    "papermill": {
     "duration": 0.026832,
     "end_time": "2021-07-27T14:50:03.624640",
     "exception": false,
     "start_time": "2021-07-27T14:50:03.597808",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Asynchronous dispatch\n",
    "\n",
    "One of the biggest differences between `ndarrays` and `DeviceArrays` is in their execution and their availability. JAX uses asynchronous dispatch to hide Python overheads. Let's take an example to understand what it means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3c648a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T14:50:03.682372Z",
     "iopub.status.busy": "2021-07-27T14:50:03.681589Z",
     "iopub.status.idle": "2021-07-27T14:50:05.032931Z",
     "shell.execute_reply": "2021-07-27T14:50:05.031686Z",
     "shell.execute_reply.started": "2021-07-21T11:34:18.778158Z"
    },
    "papermill": {
     "duration": 1.381698,
     "end_time": "2021-07-27T14:50:05.033286",
     "exception": false,
     "start_time": "2021-07-27T14:50:03.651588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of ndarray:  (8000, 8000)\n",
      "Shape of DeviceArray:  (8000, 8000)\n"
     ]
    }
   ],
   "source": [
    "# Create two random arrays sampled from a uniform distribution\n",
    "array1 = np.random.uniform(size=(8000, 8000)).astype(np.float32)\n",
    "array2 = jax.random.uniform(jax.random.PRNGKey(0), (8000, 8000), dtype=jnp.float32) # More on PRNGKey later!\n",
    "print(\"Shape of ndarray: \", array1.shape)\n",
    "print(\"Shape of DeviceArray: \", array2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede56b4d",
   "metadata": {
    "papermill": {
     "duration": 0.050665,
     "end_time": "2021-07-27T14:50:05.135088",
     "exception": false,
     "start_time": "2021-07-27T14:50:05.084423",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now, let's do some computation on each array to see what happens and how much time does each computation take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3d68834",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T14:50:05.317918Z",
     "iopub.status.busy": "2021-07-27T14:50:05.317143Z",
     "iopub.status.idle": "2021-07-27T14:50:13.165573Z",
     "shell.execute_reply": "2021-07-27T14:50:13.167262Z",
     "shell.execute_reply.started": "2021-07-21T11:34:20.204811Z"
    },
    "papermill": {
     "duration": 7.999867,
     "end_time": "2021-07-27T14:50:13.167712",
     "exception": false,
     "start_time": "2021-07-27T14:50:05.167845",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by dot product op on ndarrays: 7.94 seconds\n",
      "Time taken by dot product op on DeviceArrays: 0.02 seconds\n"
     ]
    }
   ],
   "source": [
    "# Dot product on ndarray\n",
    "start_time = time.time()\n",
    "res = np.dot(array1, array1)\n",
    "print(f\"Time taken by dot product op on ndarrays: {time.time()-start_time:.2f} seconds\")\n",
    "\n",
    "# Dot product on DeviceArray\n",
    "start_time = time.time()\n",
    "res = jnp.dot(array2, array2)\n",
    "print(f\"Time taken by dot product op on DeviceArrays: {time.time()-start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7243446",
   "metadata": {
    "papermill": {
     "duration": 0.044369,
     "end_time": "2021-07-27T14:50:13.263685",
     "exception": false,
     "start_time": "2021-07-27T14:50:13.219316",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Wow! Seems that the `DeviceArray` computation finished in no time. This is where you should remember this:\n",
    "1. Unlike the result of `ndarray`, the result of the computation done on DeviceArray isn't available yet. This is a **future** value that will be available on the accelerator \n",
    "2. You can retrieve the value of this computation by **printing** it or by converting it into a plain old numpy `ndarray`\n",
    "3. The above timing for DeviceArray is the time taken to **dispatch** the work, not the time taken for actual computation\n",
    "4. Asynchronous dispatch is useful since it allows Python code to “run ahead” of an accelerator device, keeping Python code out of the critical path. Provided the Python code enqueues work on the device faster than it can be executed, and that Python code does not actually need to inspect the output of a computation on the host, then a Python program can enqueue arbitrary amounts of work and avoid having the accelerator wait.\n",
    "5. To measure the true cost of any such operation:\n",
    "     - Either convert it to plain numpy ndarray (not preferred)\n",
    "     - Use `block_until_ready()` to wait for the computation that produced it to complete (preferred way for benchmarking)\n",
    "     \n",
    "Let's take a look at the above two methods again to measure the correct computation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e8a6b6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T14:50:13.390815Z",
     "iopub.status.busy": "2021-07-27T14:50:13.389340Z",
     "iopub.status.idle": "2021-07-27T14:50:30.884266Z",
     "shell.execute_reply": "2021-07-27T14:50:30.884787Z",
     "shell.execute_reply.started": "2021-07-21T11:34:28.089727Z"
    },
    "papermill": {
     "duration": 17.559022,
     "end_time": "2021-07-27T14:50:30.884989",
     "exception": false,
     "start_time": "2021-07-27T14:50:13.325967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 6s, sys: 459 ms, total: 1min 7s\n",
      "Wall time: 17.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1973.7642, 1957.4628, 1977.2909, ..., 1968.4293, 1975.3844,\n",
       "        1988.7894],\n",
       "       [2025.475 , 2023.9645, 2015.592 , ..., 2023.733 , 2002.3163,\n",
       "        2028.4009],\n",
       "       [2010.4509, 1999.3922, 2015.3254, ..., 2001.3368, 2002.6456,\n",
       "        1999.4705],\n",
       "       ...,\n",
       "       [1990.0709, 1980.6545, 2004.953 , ..., 2000.7068, 1989.4515,\n",
       "        1998.1526],\n",
       "       [2019.7246, 2013.85  , 2037.707 , ..., 2013.0159, 2011.6285,\n",
       "        2014.5178],\n",
       "       [2010.0378, 1999.1147, 2012.6888, ..., 2006.3755, 2002.0842,\n",
       "        2011.8866]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First we will time it by converting the computation results to ndarray\n",
    "%time np.asarray(jnp.dot(array2, array2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16d529cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T14:50:30.945217Z",
     "iopub.status.busy": "2021-07-27T14:50:30.944402Z",
     "iopub.status.idle": "2021-07-27T14:50:39.606967Z",
     "shell.execute_reply": "2021-07-27T14:50:39.606326Z",
     "shell.execute_reply.started": "2021-07-21T11:34:44.42283Z"
    },
    "papermill": {
     "duration": 8.694459,
     "end_time": "2021-07-27T14:50:39.607211",
     "exception": false,
     "start_time": "2021-07-27T14:50:30.912752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.8 s, sys: 415 ms, total: 34.3 s\n",
      "Wall time: 8.65 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([[1973.7642, 1957.4628, 1977.2909, ..., 1968.4293, 1975.3844,\n",
       "              1988.7894],\n",
       "             [2025.475 , 2023.9645, 2015.592 , ..., 2023.733 , 2002.3163,\n",
       "              2028.4009],\n",
       "             [2010.4509, 1999.3922, 2015.3254, ..., 2001.3368, 2002.6456,\n",
       "              1999.4705],\n",
       "             ...,\n",
       "             [1990.0709, 1980.6545, 2004.953 , ..., 2000.7068, 1989.4515,\n",
       "              1998.1526],\n",
       "             [2019.7246, 2013.85  , 2037.707 , ..., 2013.0159, 2011.6285,\n",
       "              2014.5178],\n",
       "             [2010.0378, 1999.1147, 2012.6888, ..., 2006.3755, 2002.0842,\n",
       "              2011.8866]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's time it using the blocking method\n",
    "%time jnp.dot(array2, array2).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8fd6be",
   "metadata": {
    "papermill": {
     "duration": 0.029162,
     "end_time": "2021-07-27T14:50:39.666297",
     "exception": false,
     "start_time": "2021-07-27T14:50:39.637135",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Types promotion\n",
    "\n",
    "This is another aspect to keep in mind. `dtype` promotion in JAX is less aggressve as compared to numpy. A few things:\n",
    "1. JAX always prefers the precision of the JAX value when promoting a Python scalar\n",
    "2. JAX always prefers the type of the floating-point or complex type when promoting an integer or boolean type against floating or complex type\n",
    "3. JAX uses floating point promotion rules that are more suited to modern accelerator devices like GPUs/TPUs\n",
    "\n",
    "Let's take an example to see these in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b62f553f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T14:50:39.731763Z",
     "iopub.status.busy": "2021-07-27T14:50:39.730596Z",
     "iopub.status.idle": "2021-07-27T14:50:39.753182Z",
     "shell.execute_reply": "2021-07-27T14:50:39.754101Z",
     "shell.execute_reply.started": "2021-07-21T11:34:52.428905Z"
    },
    "papermill": {
     "duration": 0.059886,
     "end_time": "2021-07-27T14:50:39.754360",
     "exception": false,
     "start_time": "2021-07-27T14:50:39.694474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types promotion in numpy => int64\n",
      "Types promtoion in JAX => int8\n"
     ]
    }
   ],
   "source": [
    "print(\"Types promotion in numpy =>\", end=\" \")\n",
    "print((np.int8(32) + 4).dtype)\n",
    "\n",
    "print(\"Types promtoion in JAX =>\", end=\" \")\n",
    "print((jnp.int8(32) + 4).dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7321467",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T14:50:39.817397Z",
     "iopub.status.busy": "2021-07-27T14:50:39.816365Z",
     "iopub.status.idle": "2021-07-27T14:50:40.152182Z",
     "shell.execute_reply": "2021-07-27T14:50:40.151521Z",
     "shell.execute_reply.started": "2021-07-21T11:34:52.455203Z"
    },
    "papermill": {
     "duration": 0.369089,
     "end_time": "2021-07-27T14:50:40.152348",
     "exception": false,
     "start_time": "2021-07-27T14:50:39.783259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implicit numpy casting gives:  float64\n",
      "Implicit JAX casting gives:  float32\n"
     ]
    }
   ],
   "source": [
    "array1 = np.random.randint(5, size=(2), dtype=np.int32)\n",
    "print(\"Implicit numpy casting gives: \", (array1 + 5.0).dtype)\n",
    "\n",
    "# Check the difference in semantics of the above function in JAX\n",
    "array2 = jax.random.randint(jax.random.PRNGKey(0),\n",
    "                            minval=0,\n",
    "                            maxval=5,\n",
    "                            shape=[2],\n",
    "                            dtype=jnp.int32\n",
    "                           )\n",
    "print(\"Implicit JAX casting gives: \", (array2 + 5.0).dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf43c9b",
   "metadata": {
    "papermill": {
     "duration": 0.035705,
     "end_time": "2021-07-27T14:50:40.221364",
     "exception": false,
     "start_time": "2021-07-27T14:50:40.185659",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Automatic Differentiation\n",
    "\n",
    "Automatic Differentiation is one of my favorite topics to cover. It is beautiful and demands a full tutorial. Similar to how we covered AD in depth in [TensorFlow tutorial](https://www.kaggle.com/aakashnain/tf-jax-tutorials-part3), we will cover it in-depth here in future tutorials. Here we will take a look at a simple example to see how tightly it is integrated into JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d28efed8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-27T14:50:40.293909Z",
     "iopub.status.busy": "2021-07-27T14:50:40.293136Z",
     "iopub.status.idle": "2021-07-27T14:50:40.365195Z",
     "shell.execute_reply": "2021-07-27T14:50:40.365760Z",
     "shell.execute_reply.started": "2021-07-21T11:34:59.936714Z"
    },
    "papermill": {
     "duration": 0.109614,
     "end_time": "2021-07-27T14:50:40.365971",
     "exception": false,
     "start_time": "2021-07-27T14:50:40.256357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First order gradients of y wrt x:  8.0\n",
      "Second order gradients of y wrt x:  2.0\n"
     ]
    }
   ],
   "source": [
    "def squared(x):\n",
    "    return x**2\n",
    "\n",
    "x = 4.0\n",
    "y = squared(x)\n",
    "\n",
    "dydx = jax.grad(squared)\n",
    "print(\"First order gradients of y wrt x: \", dydx(x))\n",
    "print(\"Second order gradients of y wrt x: \", jax.grad(dydx)(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545d8ba6",
   "metadata": {
    "papermill": {
     "duration": 0.029951,
     "end_time": "2021-07-27T14:50:40.425735",
     "exception": false,
     "start_time": "2021-07-27T14:50:40.395784",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "That's it for part 1! We will be looking at other things, especially the **`Automatic Differentiation`** in the next tutorial!<br>\n",
    "\n",
    "\n",
    "**References**:\n",
    "1. https://jax.readthedocs.io/en/latest/\n",
    "2. https://colinraffel.com/blog/you-don-t-know-jax.html\n",
    "3. https://flax.readthedocs.io/en/latest/notebooks/jax_for_the_impatient.html\n",
    "\n",
    "\n",
    "Let me know in the comments if you have any suggestions/queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82b5285",
   "metadata": {
    "papermill": {
     "duration": 0.036349,
     "end_time": "2021-07-27T14:50:40.498398",
     "exception": false,
     "start_time": "2021-07-27T14:50:40.462049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 51.986104,
   "end_time": "2021-07-27T14:50:42.615016",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-07-27T14:49:50.628912",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8faabb95",
   "metadata": {
    "papermill": {
     "duration": 0.023076,
     "end_time": "2021-11-05T13:18:20.079300",
     "exception": false,
     "start_time": "2021-11-05T13:18:20.056224",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/google/jax/main/images/jax_logo_250px.png\" width=\"300\" height=\"300\" align=\"center\"/><br>\n",
    "\n",
    "Welcome to another JAX tutorial. I hope you all have been enjoying the JAX Tutorials so far. If you haven't gone through the previous tutorials, I highly suggest going through them. Here are the links:\n",
    "\n",
    "1. [TF_JAX_Tutorials - Part 1](https://www.kaggle.com/aakashnain/tf-jax-tutorials-part1)\n",
    "2. [TF_JAX_Tutorials - Part 2](https://www.kaggle.com/aakashnain/tf-jax-tutorials-part2)\n",
    "3. [TF_JAX_Tutorials - Part 3](https://www.kaggle.com/aakashnain/tf-jax-tutorials-part3)\n",
    "4. [TF_JAX_Tutorials - Part 4 (JAX and DeviceArray)](https://www.kaggle.com/aakashnain/tf-jax-tutorials-part-4-jax-and-devicearray)\n",
    "5. [TF_JAX_Tutorials - Part 5 (Pure Functions in JAX)](https://www.kaggle.com/aakashnain/tf-jax-tutorials-part-5-pure-functions-in-jax/)\n",
    "6. [TF_JAX_Tutorials - Part 6 (PRNG in JAX)](https://www.kaggle.com/aakashnain/tf-jax-tutorials-part-6-prng-in-jax/)\n",
    "7. [TF_JAX_Tutorials - Part 7 (JIT in JAX)](https://www.kaggle.com/aakashnain/tf-jax-tutorials-part-7-jit-in-jax)\n",
    "8. [TF_JAX_Tutorials - Part 8 (Vmap and Pmap)](https://www.kaggle.com/aakashnain/tf-jax-tutorials-part-8-vmap-pmap)\n",
    "\n",
    "\n",
    "Today, we are going to look into another important concept `Automatic Differentiation`. We already have seen [automatic differentiation in TensorFlow](https://www.kaggle.com/aakashnain/tf-jax-tutorials-part3). The idea of automatic differentiation is pretty similar in all the frameworks, but IMO JAX does it better than all of them.\n",
    "One of the questions that people ask me generally is: If the framework already takes care of all autodiff parts, why should I bother learning about it? \n",
    "\n",
    "It is important to learn these concepts because in order to implement something that is derived from these concepts or something that isn't provided out of the box,  then you need to have a clear understanding of the underlying mechanism first.\n",
    "\n",
    "**Note:** I have already covered the fundamentals of automatic differentiation in [this](https://www.kaggle.com/aakashnain/tf-jax-tutorials-part3) notebook, hence I will skip those details here and we will directly work on examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "296d6972",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-11-05T13:18:20.131319Z",
     "iopub.status.busy": "2021-11-05T13:18:20.130757Z",
     "iopub.status.idle": "2021-11-05T13:18:21.752698Z",
     "shell.execute_reply": "2021-11-05T13:18:21.753211Z",
     "shell.execute_reply.started": "2021-11-05T12:45:33.586596Z"
    },
    "papermill": {
     "duration": 1.651121,
     "end_time": "2021-11-05T13:18:21.753568",
     "exception": false,
     "start_time": "2021-11-05T13:18:20.102447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "from jax import make_jaxpr\n",
    "from jax import vmap, pmap, jit\n",
    "from jax import grad, value_and_grad\n",
    "from jax.test_util import check_grads\n",
    "\n",
    "\n",
    "%config IPCompleter.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f701d839",
   "metadata": {
    "papermill": {
     "duration": 0.038397,
     "end_time": "2021-11-05T13:18:21.825585",
     "exception": false,
     "start_time": "2021-11-05T13:18:21.787188",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Gradients\n",
    "\n",
    "I will try to include all the examples we saw in the [TensorFlow Autodiff Notebook](https://www.kaggle.com/aakashnain/tf-jax-tutorials-part3) so that you can compare them both side by side and learn the differences between them\n",
    "\n",
    "The `grad` function in JAX is used for computing the gradients. As we know that the basic idea behind JAX is to work with function compositions,`grad` also takes a callable as input and returns a callable. So, whenever we want to do the computation of the gradients, we need to pass a callable to `grad` first. Let's take an example to make it more clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b78b89ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-05T13:18:21.892256Z",
     "iopub.status.busy": "2021-11-05T13:18:21.891600Z",
     "iopub.status.idle": "2021-11-05T13:18:22.164369Z",
     "shell.execute_reply": "2021-11-05T13:18:22.164861Z",
     "shell.execute_reply.started": "2021-11-05T12:45:35.019272Z"
    },
    "papermill": {
     "duration": 0.306589,
     "end_time": "2021-11-05T13:18:22.165022",
     "exception": false,
     "start_time": "2021-11-05T13:18:21.858433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Variable x: 3.0\n",
      "Input Variable y: 4.0\n",
      "Product z: 12.0\n",
      "\n",
      "Gradient of z wrt x: 4.0\n",
      "Gradient of z wrt y: 3.0\n"
     ]
    }
   ],
   "source": [
    "def product(x, y):\n",
    "    z = x * y\n",
    "    return z\n",
    "\n",
    "\n",
    "x = 3.0\n",
    "y = 4.0\n",
    "\n",
    "z = product(x, y)\n",
    "\n",
    "print(f\"Input Variable x: {x}\")\n",
    "print(f\"Input Variable y: {y}\")\n",
    "print(f\"Product z: {z}\\n\")\n",
    "\n",
    "# dz / dx\n",
    "dx = grad(product, argnums=0)(x, y)\n",
    "print(f\"Gradient of z wrt x: {dx}\")\n",
    "\n",
    "# dz / dy\n",
    "dy = grad(product, argnums=1)(x, y)\n",
    "print(f\"Gradient of z wrt y: {dy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3043ec55",
   "metadata": {
    "papermill": {
     "duration": 0.020969,
     "end_time": "2021-11-05T13:18:22.210239",
     "exception": false,
     "start_time": "2021-11-05T13:18:22.189270",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let us break down the above example and try to understand the gradients calculation step by step.\n",
    "\n",
    "1. We have a function named `product(...)` that takes two-position arguments as input and returns the product of these arguments\n",
    "2. We pass the `product(...)` function to `grad`  to compute the gradients. The `argnums` argument in `grad` tells `grad` to differentiate the function wrt `ith` positional argument. Hence we pass `0` and `1` for calculating the gradients wrt `x` and `y` correspondingly.\n",
    "\n",
    "You can also calculate the value of the function and the gradients in one go. For this we will use the `value_and_grad(...)` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9682637",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-05T13:18:22.253844Z",
     "iopub.status.busy": "2021-11-05T13:18:22.253356Z",
     "iopub.status.idle": "2021-11-05T13:18:22.262274Z",
     "shell.execute_reply": "2021-11-05T13:18:22.262667Z",
     "shell.execute_reply.started": "2021-11-05T12:45:35.670633Z"
    },
    "papermill": {
     "duration": 0.032177,
     "end_time": "2021-11-05T13:18:22.262829",
     "exception": false,
     "start_time": "2021-11-05T13:18:22.230652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product z: 12.0\n",
      "Gradient of z wrt x: 4.0\n"
     ]
    }
   ],
   "source": [
    "z, dx = value_and_grad(product, argnums=0)(x, y)\n",
    "print(\"Product z:\", z)\n",
    "print(f\"Gradient of z wrt x: {dx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00ae19b",
   "metadata": {
    "papermill": {
     "duration": 0.020209,
     "end_time": "2021-11-05T13:18:22.303796",
     "exception": false,
     "start_time": "2021-11-05T13:18:22.283587",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Jaxprs and `grad`\n",
    "\n",
    "As we can combine function transforms in JAX, we can make `jaxprs` from the grad function to understand what is going on behind the scene. Let's take an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee219fd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-05T13:18:22.356971Z",
     "iopub.status.busy": "2021-11-05T13:18:22.356098Z",
     "iopub.status.idle": "2021-11-05T13:18:22.371069Z",
     "shell.execute_reply": "2021-11-05T13:18:22.370637Z",
     "shell.execute_reply.started": "2021-11-05T12:45:36.677054Z"
    },
    "papermill": {
     "duration": 0.046557,
     "end_time": "2021-11-05T13:18:22.371184",
     "exception": false,
     "start_time": "2021-11-05T13:18:22.324627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Differentiating wrt x\n",
      "{ lambda  ; a b.\n",
      "  let _ = mul a b\n",
      "      c = mul 1.0 b\n",
      "  in (c,) }\n",
      "\n",
      "Differentiating wrt y\n",
      "{ lambda  ; a b.\n",
      "  let _ = mul a b\n",
      "      c = mul a 1.0\n",
      "  in (c,) }\n"
     ]
    }
   ],
   "source": [
    "# Differentiating wrt first positional argument `x`\n",
    "print(\"Differentiating wrt x\")\n",
    "print(make_jaxpr(grad(product, argnums=0))(x, y))\n",
    "\n",
    "\n",
    "# Differentiating wrt second positional argument `y`\n",
    "print(\"\\nDifferentiating wrt y\")\n",
    "print(make_jaxpr(grad(product, argnums=1))(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d61a424",
   "metadata": {
    "papermill": {
     "duration": 0.021313,
     "end_time": "2021-11-05T13:18:22.414242",
     "exception": false,
     "start_time": "2021-11-05T13:18:22.392929",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Notice that the argument other than the one wrt which we are differentiating is a constant with a value of `1`.\n",
    "\n",
    "# Stopping Gradients computation\n",
    "\n",
    "Sometimes we do not want the gradients to flow through some of the variables involved in a specific computation. In that case, we need to tell JAX explicitly that we don't want the gradients to flow through the specified set of variables. We will look into complex examples of this later on, but for now, I will modify our `product(...)` function where we do not want the gradients to flow through `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f89209e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-05T13:18:22.462425Z",
     "iopub.status.busy": "2021-11-05T13:18:22.461923Z",
     "iopub.status.idle": "2021-11-05T13:18:22.464139Z",
     "shell.execute_reply": "2021-11-05T13:18:22.464524Z",
     "shell.execute_reply.started": "2021-11-05T12:45:37.711794Z"
    },
    "papermill": {
     "duration": 0.029114,
     "end_time": "2021-11-05T13:18:22.464661",
     "exception": false,
     "start_time": "2021-11-05T13:18:22.435547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Modified product function. Explicity stopping the\n",
    "# flow of the gradients through `y`\n",
    "def product_stop_grad(x, y):\n",
    "    z = x * jax.lax.stop_gradient(y)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "394edadf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-05T13:18:22.512001Z",
     "iopub.status.busy": "2021-11-05T13:18:22.511298Z",
     "iopub.status.idle": "2021-11-05T13:18:22.528901Z",
     "shell.execute_reply": "2021-11-05T13:18:22.529291Z",
     "shell.execute_reply.started": "2021-11-05T12:45:38.248306Z"
    },
    "papermill": {
     "duration": 0.043026,
     "end_time": "2021-11-05T13:18:22.529423",
     "exception": false,
     "start_time": "2021-11-05T13:18:22.486397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(0., dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Differentiating wrt y. This should return 0\n",
    "grad(product_stop_grad, argnums=1)(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5727cdc5",
   "metadata": {
    "papermill": {
     "duration": 0.021566,
     "end_time": "2021-11-05T13:18:22.572927",
     "exception": false,
     "start_time": "2021-11-05T13:18:22.551361",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Gradients per sample\n",
    "\n",
    "In reverse mode, the gradients are defined only for a function that outputs a scalar e.g. backpropagating on your loss value to update the parameters of your machine learning model. The loss is always a scalar value. What if your function returns a batch and you want to calculate the gradients per sample for that batch? \n",
    "\n",
    "These things are pretty straightforward in JAX (thanks to `vmap` and `pmap`). In the next example, we will perform these steps in order:\n",
    "\n",
    "1. Write a function that takes an input and applies `tanh` on the input. This function is written in a way that works on a single example (Remember the philosophy behind `vmap` from the last tutorial?)\n",
    "2. We will check if we can compute the gradients on a single example\n",
    "3. We will then pass a batch of inputs and compute the gradients for the whole batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02a6e8e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-05T13:18:22.628735Z",
     "iopub.status.busy": "2021-11-05T13:18:22.620684Z",
     "iopub.status.idle": "2021-11-05T13:18:22.972754Z",
     "shell.execute_reply": "2021-11-05T13:18:22.972184Z",
     "shell.execute_reply.started": "2021-11-05T12:45:39.306548Z"
    },
    "papermill": {
     "duration": 0.377278,
     "end_time": "2021-11-05T13:18:22.972912",
     "exception": false,
     "start_time": "2021-11-05T13:18:22.595634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient for a single input x=0.5:  0.7864477\n",
      "\n",
      "Trying to compute gradients on a batch\n",
      "Input shape:  (5,)\n",
      "Output shape:  (5,)\n",
      "TypeError Gradient only defined for scalar-output functions. Output had shape: (5,).\n"
     ]
    }
   ],
   "source": [
    "def activate(x):\n",
    "    \"\"\"Applies tanh activation.\"\"\"\n",
    "    return jnp.tanh(x)\n",
    "\n",
    "\n",
    "# Check if we can compute the gradients for a single example\n",
    "grads_single_example = grad(activate)(0.5)\n",
    "print(\"Gradient for a single input x=0.5: \", grads_single_example)\n",
    "\n",
    "\n",
    "# Now we will generate a batch of random inputs, and will pass\n",
    "# those inputs to our activate function. And we will also try to\n",
    "# calculate the grads on the same batch in the same way as above\n",
    "\n",
    "# Always use the PRNG\n",
    "key = random.PRNGKey(1234)\n",
    "x = random.normal(key=key, shape=(5,))\n",
    "activations = activate(x)\n",
    "\n",
    "print(\"\\nTrying to compute gradients on a batch\")\n",
    "print(\"Input shape: \", x.shape)\n",
    "print(\"Output shape: \", activations.shape)\n",
    "\n",
    "try:\n",
    "    grads_batch = grad(activate)(x)\n",
    "    print(\"Gradients for the batch: \", grads_batch)\n",
    "except Exception as ex:\n",
    "    print(type(ex).__name__, ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e617433c",
   "metadata": {
    "papermill": {
     "duration": 0.033716,
     "end_time": "2021-11-05T13:18:23.041080",
     "exception": false,
     "start_time": "2021-11-05T13:18:23.007364",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "So what's the solution then? Well `vmap` and `pmap` is the solution to almost everything, Let's see it in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01a5d667",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-05T13:18:23.118980Z",
     "iopub.status.busy": "2021-11-05T13:18:23.112700Z",
     "iopub.status.idle": "2021-11-05T13:18:23.157554Z",
     "shell.execute_reply": "2021-11-05T13:18:23.158086Z",
     "shell.execute_reply.started": "2021-11-05T12:45:40.303129Z"
    },
    "papermill": {
     "duration": 0.082362,
     "end_time": "2021-11-05T13:18:23.158267",
     "exception": false,
     "start_time": "2021-11-05T13:18:23.075905",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients for the batch:  [0.48228705 0.45585024 0.99329686 0.0953269  0.8153717 ]\n"
     ]
    }
   ],
   "source": [
    "grads_batch = vmap(grad(activate))(x)\n",
    "print(\"Gradients for the batch: \", grads_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d60d170",
   "metadata": {
    "papermill": {
     "duration": 0.03302,
     "end_time": "2021-11-05T13:18:23.225434",
     "exception": false,
     "start_time": "2021-11-05T13:18:23.192414",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's break down all the modifications we did above to achieve the desired results.\n",
    "\n",
    "1. `grad(activate)(...)` works for a single example\n",
    "2. Adding `vmap` composition adds a batch dimension (defaults to 0) to our inputs and outputs\n",
    "\n",
    "Its' that simple to go from a single example to a batch and vice-versa. All you need is to focus on using `vmap`. Let's see how the `jaxpr` for this transformation looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23bd6000",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-05T13:18:23.302548Z",
     "iopub.status.busy": "2021-11-05T13:18:23.300403Z",
     "iopub.status.idle": "2021-11-05T13:18:23.306676Z",
     "shell.execute_reply": "2021-11-05T13:18:23.307260Z",
     "shell.execute_reply.started": "2021-11-05T12:45:41.295385Z"
    },
    "papermill": {
     "duration": 0.048521,
     "end_time": "2021-11-05T13:18:23.307423",
     "exception": false,
     "start_time": "2021-11-05T13:18:23.258902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda  ; a.\n",
       "  let b = tanh a\n",
       "      c = sub 1.0 b\n",
       "      d = mul 1.0 c\n",
       "      e = mul d b\n",
       "      f = add_any d e\n",
       "  in (f,) }"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_jaxpr(vmap(grad(activate)))(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fcf856",
   "metadata": {
    "papermill": {
     "duration": 0.033516,
     "end_time": "2021-11-05T13:18:23.374883",
     "exception": false,
     "start_time": "2021-11-05T13:18:23.341367",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Composition of other transformations\n",
    "\n",
    "We can combine any other transformation with `grad`. We already saw `vmap` applied with `grad`. Let's apply `jit` to the above transformation to make it more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82fc5daa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-05T13:18:23.456313Z",
     "iopub.status.busy": "2021-11-05T13:18:23.455614Z",
     "iopub.status.idle": "2021-11-05T13:18:23.477001Z",
     "shell.execute_reply": "2021-11-05T13:18:23.476323Z",
     "shell.execute_reply.started": "2021-11-05T12:45:42.263592Z"
    },
    "papermill": {
     "duration": 0.068469,
     "end_time": "2021-11-05T13:18:23.477157",
     "exception": false,
     "start_time": "2021-11-05T13:18:23.408688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients for the batch:  [0.48228705 0.45585027 0.99329686 0.09532695 0.8153717 ]\n",
      "Time taken: 0.02 seconds\n",
      "==================================================\n",
      "\n",
      "Gradients for the batch:  [0.48228705 0.45585027 0.99329686 0.09532695 0.8153717 ]\n",
      "Time taken: 0.00 seconds\n",
      "==================================================\n",
      "\n",
      "Gradients for the batch:  [0.48228705 0.45585027 0.99329686 0.09532695 0.8153717 ]\n",
      "Time taken: 0.00 seconds\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jitted_grads_batch = jit(vmap(grad(activate)))\n",
    "\n",
    "for _ in range(3):\n",
    "    start_time = time.time()\n",
    "    print(\"Gradients for the batch: \", jitted_grads_batch(x))\n",
    "    print(f\"Time taken: {time.time() - start_time:.2f} seconds\")\n",
    "    print(\"=\"*50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c017f89",
   "metadata": {
    "papermill": {
     "duration": 0.034174,
     "end_time": "2021-11-05T13:18:23.545989",
     "exception": false,
     "start_time": "2021-11-05T13:18:23.511815",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Validate finite differences\n",
    "Many times we want to verify the computation of the gradients with finite differences to double-check if everything we did is right. Because this is a pretty-common sanity check while working with derivatives, JAX provides a convenient function `check_grads` that checks the finite differences to any order of gradients. Let's take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9e7bce8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-05T13:18:23.618355Z",
     "iopub.status.busy": "2021-11-05T13:18:23.617765Z",
     "iopub.status.idle": "2021-11-05T13:18:23.708220Z",
     "shell.execute_reply": "2021-11-05T13:18:23.708938Z",
     "shell.execute_reply.started": "2021-11-05T12:45:43.216946Z"
    },
    "papermill": {
     "duration": 0.128827,
     "end_time": "2021-11-05T13:18:23.709154",
     "exception": false,
     "start_time": "2021-11-05T13:18:23.580327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient match with gradient calculated using finite differences\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    check_grads(jitted_grads_batch, (x,),  order=1)\n",
    "    print(\"Gradient match with gradient calculated using finite differences\")\n",
    "except Exception as ex:\n",
    "    print(type(ex).__name__, ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78636e1e",
   "metadata": {
    "papermill": {
     "duration": 0.034278,
     "end_time": "2021-11-05T13:18:23.778534",
     "exception": false,
     "start_time": "2021-11-05T13:18:23.744256",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Higher Order Gradients\n",
    "\n",
    "`grad` function takes a callable as an input and returns another function. We can compose the function returned by the transformation with `grad` again and again to compute higher-order derivates of any order. Let's take an example to see it in action. We will use our `activate(...)` function to demonstrate this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c9ebe3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-05T13:18:23.850664Z",
     "iopub.status.busy": "2021-11-05T13:18:23.850059Z",
     "iopub.status.idle": "2021-11-05T13:18:24.066423Z",
     "shell.execute_reply": "2021-11-05T13:18:24.066970Z",
     "shell.execute_reply.started": "2021-11-05T12:45:44.161249Z"
    },
    "papermill": {
     "duration": 0.254781,
     "end_time": "2021-11-05T13:18:24.067140",
     "exception": false,
     "start_time": "2021-11-05T13:18:23.812359",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First order derivative:  0.7864477\n",
      "Second order derivative:  -0.726862\n",
      "Third order derivative:  -0.5652091\n"
     ]
    }
   ],
   "source": [
    "x = 0.5\n",
    "\n",
    "print(\"First order derivative: \", grad(activate)(x))\n",
    "print(\"Second order derivative: \", grad(grad(activate))(x))\n",
    "print(\"Third order derivative: \", grad(grad(grad(activate)))(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127a7089",
   "metadata": {
    "papermill": {
     "duration": 0.037029,
     "end_time": "2021-11-05T13:18:24.140169",
     "exception": false,
     "start_time": "2021-11-05T13:18:24.103140",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Gradients and numerical stability\n",
    "\n",
    "`Underflow` and `Overflow` are common problems that we run into many times especially while computing the gradients. We will take an example (this one is straight from the JAX docs, and it's a pretty good example) to illustrate how we can run into numerical instability and how JAX tries to aid you to overcome it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2eb9ca79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-05T13:18:24.218376Z",
     "iopub.status.busy": "2021-11-05T13:18:24.216398Z",
     "iopub.status.idle": "2021-11-05T13:18:24.220154Z",
     "shell.execute_reply": "2021-11-05T13:18:24.220737Z",
     "shell.execute_reply.started": "2021-11-05T12:45:45.054267Z"
    },
    "papermill": {
     "duration": 0.044101,
     "end_time": "2021-11-05T13:18:24.220919",
     "exception": false,
     "start_time": "2021-11-05T13:18:24.176818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# An example of a mathematical operation in your workflow\n",
    "def log1pexp(x):\n",
    "    \"\"\"Implements log(1 + exp(x))\"\"\"\n",
    "    return jnp.log(1. + jnp.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86328b7",
   "metadata": {
    "papermill": {
     "duration": 0.03632,
     "end_time": "2021-11-05T13:18:24.293643",
     "exception": false,
     "start_time": "2021-11-05T13:18:24.257323",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "What happens when you compute the gradients for some value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e01b970e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-05T13:18:24.369811Z",
     "iopub.status.busy": "2021-11-05T13:18:24.369135Z",
     "iopub.status.idle": "2021-11-05T13:18:24.438760Z",
     "shell.execute_reply": "2021-11-05T13:18:24.438161Z",
     "shell.execute_reply.started": "2021-11-05T12:45:47.944820Z"
    },
    "papermill": {
     "duration": 0.108789,
     "end_time": "2021-11-05T13:18:24.438919",
     "exception": false,
     "start_time": "2021-11-05T13:18:24.330130",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients for a small value of x:  0.9933072\n"
     ]
    }
   ],
   "source": [
    "# This works fine\n",
    "print(\"Gradients for a small value of x: \", grad(log1pexp)(5.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "662f478c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-05T13:18:24.516318Z",
     "iopub.status.busy": "2021-11-05T13:18:24.515452Z",
     "iopub.status.idle": "2021-11-05T13:18:24.524472Z",
     "shell.execute_reply": "2021-11-05T13:18:24.523811Z",
     "shell.execute_reply.started": "2021-11-05T12:45:48.797318Z"
    },
    "papermill": {
     "duration": 0.049438,
     "end_time": "2021-11-05T13:18:24.524662",
     "exception": false,
     "start_time": "2021-11-05T13:18:24.475224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients for a large value of x:  nan\n"
     ]
    }
   ],
   "source": [
    "# But what about for very large values of x for which the\n",
    "# exponent operation will explode\n",
    "print(\"Gradients for a large value of x: \", grad(log1pexp)(500.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e1fb2e",
   "metadata": {
    "papermill": {
     "duration": 0.036148,
     "end_time": "2021-11-05T13:18:24.598215",
     "exception": false,
     "start_time": "2021-11-05T13:18:24.562067",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Woah! What just happened? Let's break it down to understand the expected output and what is gpoing on behind the scene in JAX that returned `nan`. We know that derivative of the above function can be written like this:\n",
    "\n",
    "<div align=\"center\">$\\frac{\\mathrm{d} }{\\mathrm{d} x}(log(1 + e^{x}) = \\frac {e^{x}} {1 + e^{x}}$</div><br>\n",
    "\n",
    "For very large values, you would expect the value of the derivative to be 1 but when we combined `grad` with our function implementation, it returned `nan`. To get more insights, we can break down the gradients computation by looking at the `jaxpr` of the transformation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9341d6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-05T13:18:24.679193Z",
     "iopub.status.busy": "2021-11-05T13:18:24.674027Z",
     "iopub.status.idle": "2021-11-05T13:18:24.686009Z",
     "shell.execute_reply": "2021-11-05T13:18:24.686616Z",
     "shell.execute_reply.started": "2021-11-05T12:45:54.626175Z"
    },
    "papermill": {
     "duration": 0.052199,
     "end_time": "2021-11-05T13:18:24.686793",
     "exception": false,
     "start_time": "2021-11-05T13:18:24.634594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda  ; a.\n",
       "  let b = exp a\n",
       "      c = add b 1.0\n",
       "      _ = log c\n",
       "      d = div 1.0 c\n",
       "      e = mul d b\n",
       "  in (e,) }"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_jaxpr(grad(log1pexp))(500.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6d518b",
   "metadata": {
    "papermill": {
     "duration": 0.036483,
     "end_time": "2021-11-05T13:18:24.759788",
     "exception": false,
     "start_time": "2021-11-05T13:18:24.723305",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "If you take a closer look, you will notice that the computation is equivalent to this:\n",
    "\n",
    "<div align=\"center\">$\\frac{1}{1 + e^{x}} * e^{x}$</div><br>\n",
    "\n",
    "For large values, the term on the right-hand side will be rounded off to `inf`, and the `grad` computation will return `nan` as we saw above. A human knows how to compute the gradient correctly in this case but JAX doesn't. It is working on the standard autodiff rules. So, how do we tell JAX that our function should be differentiated in the way we want? We can achieve this using `custom_vjp` or `custom_vjp` functions in JAX. Let's see it in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5740570",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-05T13:18:24.836653Z",
     "iopub.status.busy": "2021-11-05T13:18:24.836036Z",
     "iopub.status.idle": "2021-11-05T13:18:24.842368Z",
     "shell.execute_reply": "2021-11-05T13:18:24.841950Z",
     "shell.execute_reply.started": "2021-11-05T12:46:03.960634Z"
    },
    "papermill": {
     "duration": 0.045515,
     "end_time": "2021-11-05T13:18:24.842474",
     "exception": false,
     "start_time": "2021-11-05T13:18:24.796959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from jax import custom_jvp\n",
    "\n",
    "@custom_jvp\n",
    "def log1pexp(x):\n",
    "    \"\"\"Implements log(1 + exp(x))\"\"\"\n",
    "    return jnp.log(1. + jnp.exp(x))\n",
    "\n",
    "@log1pexp.defjvp\n",
    "def log1pexp_jvp(primals, tangents):\n",
    "    \"\"\"Tells JAX to differentiate the function in the way we want.\"\"\"\n",
    "    x, = primals\n",
    "    x_dot, = tangents\n",
    "    ans = log1pexp(x)\n",
    "    # This is where we define the correct way to compute gradients\n",
    "    ans_dot = (1 - 1/(1 + jnp.exp(x))) * x_dot\n",
    "    return ans, ans_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f64d2f9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-05T13:18:24.902687Z",
     "iopub.status.busy": "2021-11-05T13:18:24.901810Z",
     "iopub.status.idle": "2021-11-05T13:18:24.978674Z",
     "shell.execute_reply": "2021-11-05T13:18:24.977968Z",
     "shell.execute_reply.started": "2021-11-05T12:46:05.018596Z"
    },
    "papermill": {
     "duration": 0.109051,
     "end_time": "2021-11-05T13:18:24.978814",
     "exception": false,
     "start_time": "2021-11-05T13:18:24.869763",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients for a small value of x:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Let's now compute the gradients for large values\n",
    "print(\"Gradients for a small value of x: \", grad(log1pexp)(500.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65cfe32d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-05T13:18:25.064311Z",
     "iopub.status.busy": "2021-11-05T13:18:25.057967Z",
     "iopub.status.idle": "2021-11-05T13:18:25.073780Z",
     "shell.execute_reply": "2021-11-05T13:18:25.074331Z",
     "shell.execute_reply.started": "2021-11-05T12:47:17.292823Z"
    },
    "papermill": {
     "duration": 0.057611,
     "end_time": "2021-11-05T13:18:25.074522",
     "exception": false,
     "start_time": "2021-11-05T13:18:25.016911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda  ; a.\n",
       "  let _ = custom_jvp_call_jaxpr[ fun_jaxpr={ lambda  ; a.\n",
       "                                             let b = exp a\n",
       "                                                 c = add b 1.0\n",
       "                                                 d = log c\n",
       "                                             in (d,) }\n",
       "                                 jvp_jaxpr_thunk=<function _memoize.<locals>.memoized at 0x7f99201a3200>\n",
       "                                 num_consts=0 ] a\n",
       "      b = exp a\n",
       "      c = add b 1.0\n",
       "      d = div 1.0 c\n",
       "      e = sub 1.0 d\n",
       "      f = mul e 1.0\n",
       "  in (f,) }"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What about the Jaxpr?\n",
    "make_jaxpr(grad(log1pexp))(500.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9013186e",
   "metadata": {
    "papermill": {
     "duration": 0.037717,
     "end_time": "2021-11-05T13:18:25.149892",
     "exception": false,
     "start_time": "2021-11-05T13:18:25.112175",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's break down the steps we did to achieve the expected results.\n",
    "\n",
    "1. We decorated our `log1pexp(...)` with `custom_vjp` that computes the Jacobian-vector product (forward-mode)\n",
    "2. we then defined `log1pexp_jvp(...)` that defines how the gradients should be computed. Focus on this line of code in that function: `ans_dot = (1 - 1/(1 + jnp.exp(x))) * x_dot`. Simply written, all we are doing is to rearrange the derivative in this way:\n",
    "\n",
    "<div align=\"center\">$\\frac{\\mathrm{d} }{\\mathrm{d} x}(log(1 + e^{x}) = 1 - \\frac {1} {1 + e^{x}}$</div><br>\n",
    "\n",
    "\n",
    "3. We decorate the `logp1exp_jvp(...)` function with `log1pexp.defjvp` to tell JAX that for calculating JVP, please consume the function we have defined and return the expected output\n",
    "\n",
    "\n",
    "That's it for this tutorial folks! Many things were left out of this tutorial on purpose. For example, we didn't cover the forward-mode, and reverse-mode in detail because the scope of those concepts is outside of this notebook. If you want to understand those concepts, I highly suggest going through this [Advanced Autodiff Documentation](https://jax.readthedocs.io/en/latest/jax-101/04-advanced-autodiff.html). There is one last fundamental concept remaining for the JAX series that we will be covering in the upcoming tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d18fafb",
   "metadata": {
    "papermill": {
     "duration": 0.037733,
     "end_time": "2021-11-05T13:18:25.225297",
     "exception": false,
     "start_time": "2021-11-05T13:18:25.187564",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# References\n",
    "\n",
    "1. https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html\n",
    "2. https://jax.readthedocs.io/en/latest/jax-101/04-advanced-autodiff.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3848dd",
   "metadata": {
    "papermill": {
     "duration": 0.037033,
     "end_time": "2021-11-05T13:18:25.299808",
     "exception": false,
     "start_time": "2021-11-05T13:18:25.262775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15.952425,
   "end_time": "2021-11-05T13:18:28.208164",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-11-05T13:18:12.255739",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
